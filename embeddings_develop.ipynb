{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing DQN with embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Convex Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward pass computes $Q(s,a)$ and maximizes it by one of inputs using L-BFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn.model import ConvexQNetwork\n",
    "\n",
    "\n",
    "model = ConvexQNetwork(\n",
    "    state_size=300,\n",
    "    emb_size=3,\n",
    "    hidden_size=256,\n",
    "    optim_steps=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "qfunc, a = model.forward(torch.randn(3, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.0559],\n",
       "        [10.5624],\n",
       "        [ 2.9934]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qfunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -0.9577,  -0.5398,  -3.1173],\n",
       "        [ -9.1446,  -9.7397, -10.8742],\n",
       "        [ -1.4427,  -1.6735,  -4.2116]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[7.000731468200684,\n",
       "  -0.034524381160736084,\n",
       "  -0.12732195854187012,\n",
       "  -0.22011947631835938,\n",
       "  -0.3129172921180725,\n",
       "  -0.4057149887084961,\n",
       "  -0.49851250648498535,\n",
       "  -0.5913101434707642,\n",
       "  -0.6718336343765259,\n",
       "  -0.8345907926559448,\n",
       "  -0.9973480701446533,\n",
       "  -1.1601054668426514,\n",
       "  -1.3228628635406494,\n",
       "  -1.4856199026107788,\n",
       "  -1.6483771800994873,\n",
       "  -1.7574591636657715,\n",
       "  -1.8171570301055908,\n",
       "  -1.876854658126831,\n",
       "  -1.9365522861480713,\n",
       "  -1.9962501525878906],\n",
       " [319.8938903808594,\n",
       "  197.17987060546875,\n",
       "  -9.929808616638184,\n",
       "  -9.96495532989502,\n",
       "  -10.000100135803223,\n",
       "  -10.035244941711426,\n",
       "  -10.070390701293945,\n",
       "  -10.105536460876465,\n",
       "  -10.140681266784668,\n",
       "  -10.175826072692871,\n",
       "  -10.210970878601074,\n",
       "  -10.246116638183594,\n",
       "  -10.281262397766113,\n",
       "  -10.316407203674316,\n",
       "  -10.35155200958252,\n",
       "  -10.386697769165039,\n",
       "  -10.421843528747559,\n",
       "  -10.456988334655762,\n",
       "  -10.492134094238281,\n",
       "  -10.527278900146484],\n",
       " [13.005784034729004,\n",
       "  1.4321315288543701,\n",
       "  0.3737785220146179,\n",
       "  -1.0266350507736206,\n",
       "  -1.242457389831543,\n",
       "  -1.365346074104309,\n",
       "  -1.4738857746124268,\n",
       "  -1.582425594329834,\n",
       "  -1.6909654140472412,\n",
       "  -1.7995051145553589,\n",
       "  -1.908044695854187,\n",
       "  -2.016584634780884,\n",
       "  -2.125124216079712,\n",
       "  -2.23366379737854,\n",
       "  -2.3422036170959473,\n",
       "  -2.4507434368133545,\n",
       "  -2.5592830181121826,\n",
       "  -2.6678225994110107,\n",
       "  -2.776362180709839,\n",
       "  -2.884902000427246]]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "by given word predict another word with shared letters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.action import WordPairsDataset\n",
    "from wordle.wordlenp import Wordle\n",
    "import numpy as np\n",
    "\n",
    "vocabulary = Wordle._load_vocabulary('wordle/guesses.txt', astype=np.array)\n",
    "data = WordPairsDataset(vocabulary, 'word_pairs_dataset', generate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "204834634"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), data[2]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from environment.action import Embedding\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = Embedding().to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss(reduction='sum')\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset=data,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "model.train_epoch(dataloader, loss_fn, optimizer, device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'embedding_model.pth')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LETS GO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "\n",
    "from wordle.wordlenp import Wordle\n",
    "from environment.environment import Environment, StateYesNo, StateVocabulary\n",
    "from environment.action import ActionEmbedding, ActionLetters\n",
    "from dqn.agent import Agent\n",
    "from dqn.train import Trainer\n",
    "from replay_buffer.cpprb import PrioritizedReplayBuffer, ReplayBuffer\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "import torch\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "\"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = Wordle._load_vocabulary('wordle/guesses.txt', astype=np.array)\n",
    "\n",
    "def make_data(n_answers, n_guesses):\n",
    "    guesses = np.random.choice(word_list, size=n_guesses, replace=False)\n",
    "    answers = np.random.choice(guesses, size=n_answers, replace=False)\n",
    "    return answers, guesses\n",
    "\n",
    "answers_10_100, guesses_10_100 = make_data(10, 100)\n",
    "answers_100_100, guesses_100_100 = make_data(100, 100)\n",
    "step_rewards = {'B':0, 'Y':1, 'G':1, 'win':10, 'lose':-10, 'step':-5}\n",
    "tasks_results = defaultdict(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(answers, guesses, n_batches, n_batches_warm):\n",
    "    env_list = []\n",
    "    for _ in range(8):\n",
    "        env = Environment(\n",
    "            rewards=step_rewards,\n",
    "            wordle=Wordle(vocabulary=guesses, answers=answers),\n",
    "            state_instance=StateYesNo()\n",
    "        )\n",
    "        env_list.append(env)\n",
    "\n",
    "    agent = Agent(\n",
    "        state_size=env.state.size,\n",
    "        action_instance=ActionEmbedding(vocabulary=guesses, emb_size=10),\n",
    "        replay_buffer=ReplayBuffer(state_size=env.state.size, batch_size=4),\n",
    "        optimize_interval=2\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        env_list[0], agent,\n",
    "        n_batches=n_batches,\n",
    "        n_batches_warm=n_batches_warm,\n",
    "        is_parallel=False,\n",
    "    )\n",
    "    \n",
    "    res = trainer.train(eps_decay=0.99, nickname=f'embtest-{len(answers)}-{len(guesses)}')\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a22118dd876c48d28ec3ab6387659612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "WARM BATCHES:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04fe3f9a855a49d281ac0d74a5f8502e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "TRAIN BATCHES:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/repos/wordle-rl/dqn/train.py:323: RuntimeWarning: Mean of empty slice.\n",
      "  mean_steps = steps[success.astype(bool)].mean()\n",
      "/home/ilya/.local/lib/python3.10/site-packages/numpy/core/_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Batch   25\tTime: 12 s\tAgent Eps: 0.78\tTrain Win Rate: 0.00%\tTest Win Rate: 0.00%\tTest Mean Steps: nan\n",
      "\n",
      "Batch   50\tTime: 26 s\tAgent Eps: 0.61\tTrain Win Rate: 4.00%\tTest Win Rate: 0.00%\tTest Mean Steps: nan\n",
      "\n",
      "Batch   75\tTime: 41 s\tAgent Eps: 0.47\tTrain Win Rate: 4.00%\tTest Win Rate: 0.00%\tTest Mean Steps: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ilya/repos/wordle-rl/embeddings_develop.ipynb Cell 24\u001b[0m in \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m experiment(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     answers_10_100, guesses_10_100,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     n_batches\u001b[39m=\u001b[39;49m\u001b[39m200\u001b[39;49m, n_batches_warm\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m )\n",
      "\u001b[1;32m/home/ilya/repos/wordle-rl/embeddings_develop.ipynb Cell 24\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m agent \u001b[39m=\u001b[39m Agent(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     state_size\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39msize,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     action_instance\u001b[39m=\u001b[39mActionEmbedding(vocabulary\u001b[39m=\u001b[39mguesses, emb_size\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     tau\u001b[39m=\u001b[39m\u001b[39m1e-5\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     env_list[\u001b[39m0\u001b[39m], agent,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     n_batches\u001b[39m=\u001b[39mn_batches,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     is_parallel\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m res \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39;49mtrain(eps_decay\u001b[39m=\u001b[39;49m\u001b[39m0.99\u001b[39;49m, nickname\u001b[39m=\u001b[39;49m\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39membtest-\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mlen\u001b[39;49m(answers)\u001b[39m}\u001b[39;49;00m\u001b[39m-\u001b[39;49m\u001b[39m{\u001b[39;49;00m\u001b[39mlen\u001b[39;49m(guesses)\u001b[39m}\u001b[39;49;00m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ilya/repos/wordle-rl/embeddings_develop.ipynb#X53sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mreturn\u001b[39;00m res\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/train.py:96\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, eps_start, eps_end, eps_decay, nickname)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mfor\u001b[39;00m i_batch \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_batches\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m), desc\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTRAIN BATCHES\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[39m# collect batch of replays\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39meval \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m     batch_scores, batch_wins \u001b[39m=\u001b[39m play_batch()\n\u001b[1;32m     98\u001b[0m     \u001b[39m# decrease exploration chance\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39meps \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(eps_end, eps_decay \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39meps)\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/train.py:192\u001b[0m, in \u001b[0;36mTrainer.play_batch_successively\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39m# play until batch of replays is collected\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[39mwhile\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mn_seen \u001b[39m-\u001b[39m before \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mplay_batch_size):\n\u001b[1;32m    190\u001b[0m     \n\u001b[1;32m    191\u001b[0m     \u001b[39m# play single episode\u001b[39;00m\n\u001b[0;32m--> 192\u001b[0m     episode_score, episode_win \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay_episode()\n\u001b[1;32m    194\u001b[0m     \u001b[39m# collect stats\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     batch_scores\u001b[39m.\u001b[39mappend(episode_score)\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/train.py:222\u001b[0m, in \u001b[0;36mTrainer.play_episode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m next_state, reward, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m    221\u001b[0m \u001b[39m# collect replay\u001b[39;00m\n\u001b[0;32m--> 222\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49madd(state, action, reward, next_state, done)\n\u001b[1;32m    224\u001b[0m \u001b[39m# collect reward\u001b[39;00m\n\u001b[1;32m    225\u001b[0m score \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/agent.py:83\u001b[0m, in \u001b[0;36mAgent.add\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mt \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_interval \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearn()\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/agent.py:197\u001b[0m, in \u001b[0;36mAgent.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m     q_target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction(nn_output)\u001b[39m.\u001b[39mqfunc\n\u001b[1;32m    196\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m     q_target, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mqnetwork_target(batch[\u001b[39m'\u001b[39;49m\u001b[39mnext_state\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    199\u001b[0m \u001b[39m# discounted return\u001b[39;00m\n\u001b[1;32m    200\u001b[0m expected_values \u001b[39m=\u001b[39m batch[\u001b[39m'\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mn_step \u001b[39m*\u001b[39m q_target \u001b[39m*\u001b[39m (\u001b[39m~\u001b[39mbatch[\u001b[39m'\u001b[39m\u001b[39mdone\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/model.py:73\u001b[0m, in \u001b[0;36mConvexQNetwork.forward\u001b[0;34m(self, s, a)\u001b[0m\n\u001b[1;32m     70\u001b[0m     param\u001b[39m.\u001b[39mrequires_grad \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[39m# L-BFGS optimization\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m a, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv \u001b[39m=\u001b[39m Optimizer(minus_Q, s, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim_steps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memb_size)\u001b[39m.\u001b[39;49msolve()\n\u001b[1;32m     75\u001b[0m \u001b[39m# enable gradients back\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/model.py:125\u001b[0m, in \u001b[0;36mOptimizer.solve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# start optimization algorithm\u001b[39;00m\n\u001b[1;32m    124\u001b[0m closure \u001b[39m=\u001b[39m partial(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_closure, s\u001b[39m=\u001b[39ms, a\u001b[39m=\u001b[39ma, conv\u001b[39m=\u001b[39mconv[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], optimizer\u001b[39m=\u001b[39moptimizer)\n\u001b[0;32m--> 125\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep(closure)\n\u001b[1;32m    127\u001b[0m \u001b[39m# collect resulting action\u001b[39;00m\n\u001b[1;32m    128\u001b[0m res\u001b[39m.\u001b[39mappend(a\u001b[39m.\u001b[39mdetach())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/lbfgs.py:438\u001b[0m, in \u001b[0;36mLBFGS.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[39mif\u001b[39;00m n_iter \u001b[39m!=\u001b[39m max_iter:\n\u001b[1;32m    434\u001b[0m     \u001b[39m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[39m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[39m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39menable_grad():\n\u001b[0;32m--> 438\u001b[0m         loss \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(closure())\n\u001b[1;32m    439\u001b[0m     flat_grad \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gather_flat_grad()\n\u001b[1;32m    440\u001b[0m     opt_cond \u001b[39m=\u001b[39m flat_grad\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax() \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m tolerance_grad\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/model.py:95\u001b[0m, in \u001b[0;36mOptimizer._closure\u001b[0;34m(self, s, a, conv, optimizer)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[39mArgument for torch.optim.LBFGS.step\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[39m# forward pass to compute objective\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m obj \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mobj(s, a)\n\u001b[1;32m     96\u001b[0m conv\u001b[39m.\u001b[39mappend(obj\u001b[39m.\u001b[39mitem())\n\u001b[1;32m     98\u001b[0m \u001b[39m# delete previous gradients wrt self.a[closure.ind]\u001b[39;00m\n",
      "File \u001b[0;32m~/repos/wordle-rl/dqn/model.py:61\u001b[0m, in \u001b[0;36mConvexQNetwork.forward.<locals>.minus_Q\u001b[0;34m(s, a)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminus_Q\u001b[39m(s, a):\n\u001b[0;32m---> 61\u001b[0m     u1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWs(s))\n\u001b[1;32m     62\u001b[0m     z1 \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWz_1(a) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWu_1(s))\n\u001b[1;32m     63\u001b[0m     z2 \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWu_2(u1) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWz_2(z1) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWa(a)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/functional.py:1446\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1434\u001b[0m threshold \u001b[39m=\u001b[39m _threshold\n\u001b[1;32m   1436\u001b[0m threshold_ \u001b[39m=\u001b[39m _add_docstr(\n\u001b[1;32m   1437\u001b[0m     _VF\u001b[39m.\u001b[39mthreshold_,\n\u001b[1;32m   1438\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1442\u001b[0m \u001b[39m\"\"\"\u001b[39;00m,\n\u001b[1;32m   1443\u001b[0m )\n\u001b[0;32m-> 1446\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrelu\u001b[39m(\u001b[39minput\u001b[39m: Tensor, inplace: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m   1447\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"relu(input, inplace=False) -> Tensor\u001b[39;00m\n\u001b[1;32m   1448\u001b[0m \n\u001b[1;32m   1449\u001b[0m \u001b[39m    Applies the rectified linear unit function element-wise. See\u001b[39;00m\n\u001b[1;32m   1450\u001b[0m \u001b[39m    :class:`~torch.nn.ReLU` for more details.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1452\u001b[0m     \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39minput\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "experiment(\n",
    "    answers_10_100, guesses_10_100,\n",
    "    n_batches=200, n_batches_warm=50,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
