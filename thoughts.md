# Идеи и планы

## Эмбеддинги для списка слов (пространства действий)

### Решение andrewkho@git

Предлагается обучить Q-сеть предсказывать не сразу вектор из 13K значений Q-функции, а предсказывать вектор из 26*5 значений: сеть предсказывает ценность каждой из 26 букв алфавита быть на каждой из 5 позиций в слове. Тогда значение Q-функции для данного слова при данном состоянии среды определяется как сумма ценностей его букв.

Более строго. Пусть
- `alph` --- алфавит размера $m=26$
- `guesses` --- список длины $n=12972$ валидных слов из $q=5$ букв
- $W\in\{0,1\}^{n\times mq}$ --- OneHot-кодированный список слов, где `W[i, j*26+k] = guesses[i][j] == alph[k]`, т.е. индикатор того, что $j$-ая буква $i$-го слова является $k$-ой буквой алфавита
- $s$ --- состояние среды, $a_\theta(s)\in\mathbb{R}^{mq}$ --- выход нейросети.

Тогда значения $Q$-функции вычисляются как
$$
Q(s)=Wa_\theta(s)\in\mathbb{R}^{n}.
$$

Математически идею такого решения можно описать так. Пусть $\mathcal{A}$ --- пространство действий. Необходимо найти отображение $d:\mathcal{A}\to\widetilde{\mathcal{A}}$ такое, что $\dim\mathcal{A}>\dim\widetilde{\mathcal{A}}$ и $Q(s,a)=\lambda()$

Идею такого решения можно описать так: представить каждое действие агента как сумму некоторых других действий, тогда ценность исходного действия равна сумме ценностей поддействий. В данном случае выбор слова был представлен как результат выборов каждой его буквы по отдельности. Главное, чтобы разбиение действия на поддействия можно было реализовать дифференцируемыми операциями.

У решения andrewkho@git один недостаток. Цель начальных ходов в Wordle --- получить информацию о буквах и позициях, которые еще не вводились ранее, значит, стоит определять поддействия как это самое тестирование. Решение andrewkho@git тестирует комбинацию (буква, позиция), но нет связи между (буква1, позиция1) и (буква2, позиция2). В связи с этим вот идеи как бить слово так, чтобы учесть взаимодействие:
- Пары соседних букв: `where -> wh, he, er, re`
    - количество возможных пар букв равно $m^2=676$, всего позиций $q-1$, значит размер эмбеддинга $h=(q-1)m^2=4\cdot676=2704$
    - минус: в таком решении позиция1 и позиция2 обязательно соседние, кажется что это большое ограничение, идеальный игрок должен уметь тестировать любые пары позиций
- Все пары букв: `where -> wh, we, wr, we, he, hr, he, er, ee, re`
    - способов выбрать пару ${q\choose 2}=0.5q(q-1)$, поэтому размерность эмбеддинга $h=0.5q(q-1)m^2=0.5\cdot5\cdot4\cdot676=6760$
    - это решение мне пока кажется идеальным (из-за объяснений в терминах буква1, позиция1)

Описанное решение обладает только одним недостатком: размер эмбеддинга растет квадратично по размеру алфавита и по длине слова. Например, если играть с русскими словами из 10 букв, то эмбеддинг потребует $0.5\cdot10\cdot9\cdot32^2=46080$ (при том что плюс-минус общеупотребительных слов из 10 букв на русском языке ненамного больше?).

Рассмотрим вопрос универсальности. Такой эмбеддинг позволит агенту обучиться решать задачу игры в Wordle весьма обобщенно: ведь теперь он учится не просто выбирать слово из списка, а тестировать буквы и позиции. Поэтому, если к списку тестовых слов добавить новое, то чтобы найти его функцию ценности достаточно добавить строку его OneHot-кода в матрицу $W$. Это значит, что агент сможет угадывать слова, которые ранее не видел. Правда по-хорошему после усложнения задачи добавлением слов в идеале нужно дообучиться, т.к. функция ценности, аппроксимируемая нейросетью, уделяет должное внимание только тем сочетаниям букв и позиций, которые встречались при обучении.

На описанное решение можно посмотреть как на архитектуру кодировщик---декодировщик: кодировщиком является нейросеть, которая выдает вектор ценности для поддействий длины $h$, а декодировщиком --- матрица OneHot-кодов, выдающая вектор длины $n$, по сути линейный слой.

Можно ли обучить нейросеть бить действие на поддействия? Что мы требуем: чтобы она выдавала вектор длины $h$, и сама его декодировала. По сути для этого надо в обычной полносвязной нейросети с $n$ выходами убрать функцию активации перед последним слоем.

Можно попытаться придумать декодировщик, но как мне кажется, любое решение где на выходе стоит $n$ чисел не стоит того, чтобы продолжать о нем говорить. Пора думать об идее Кропотова: сеть которая осуществляет преобразование вида

$$
a_\theta(s)=Q(s, \arg\max_eQ(s,a(e))),
$$

где $e$ --- эмбеддинг, $a(e)$ --- слово, ближайшее к данному эмбеддингу.
- Режим теста. Вход $e$ будет в некотором смысле пустышкой при запуске теста, а после завершения теста в нем уже будет храниться эмбединг-ответ.
    1. Проход вперед с входом $s,e_0\to$ получили значение $Q(s,e_0)$.
    2. Затем нужен проход назад, чтобы пробросить градиент до входа $e$, при этом нужно отключить обновление параметров сети.
    3. Получили градиент по $e$ --- делаем один шаг оптимизации и получаем $e_1$.
    4. Подаем $s,e_1$ на вход сети для прохода вперед, получаем $Q(s,e_1)$ повторяем шаги 2-3. Так делаем, к примеру, 20 раз методом L-BFGS.
    5. Выбираем действие $a(e)$
- Режим обучения. 
    1. Делаем все то же самое
    2. Для полученного $Q(s,e^*)$ считаем MSE (согласно архитектуре DQN) и теперь уже обновляем параметры сети.

## Исследование архитектуры сети

Параметры, которые нужно подобрать:
1. посмотреть число слоев
2. добавить батчнормы

### попробовать оптюну

- плюс: красивые графики и простая интерпретация, высокая точность оптимизации
- минус: это будет считаться миллион времени, НО: мейби можно придумать какой-то простой и быстрый бенчмарк (например, аккураси на тестовом геймплее 100/100 после одной минуты обучения)

### использовать эталонное решение

1. реализовать решение 3blue1brown
2. нагенерить буфер реплеев
3. к распределениям q-функции применить софтмакс
4. действия эталона считать как вырожденное распределение
5. обучить на кросс-энтропию

- это можно использовать как еще один бенчмарк в оптюне, который косвенно проверяет что вообще способна выучить такая архитектура (например, аккураси на тестовом геймплее ПОЛНОЙ ЗАДАЧИ после минуты обучения)
- минус: надо реализовать...
- минус: это долго реализовывать...

## Игры батчами

нужно обеспечить баланс между количеством поступающего опыта (replays) и размером батча обучения
- если опыта будет поступать меньше чем один батч, то сеть будет обучаться на одних и тех же примерах много раз
- если опыта будет поступать больше чем один батч, то сеть будет просто не успевать учиться на всех примерах и память будет тратиться в пустую

### исходная реализация

- игры проходят последовательно, на каждом шаге функция `agent.act` вызывается для одного стейта, обучение через каждые два шага
- минус: нет баланса реплеев и обучения
- минус: медленно (из-за `agent.act`)
- плюс: обучается и дает 90% на задаче 100/100
- статус: отказался
- вывод: баланс действительно нужен

### последовательный способ

- играть последовательно столько игр, чтобы набралось `batch_size` реплеев, и после этого обучать
- плюс: баланс опыта и обучения
- минус: функция `agent.act` все равно обрабатывает не батч стейтов, а последовательно каждый стейт
- минус: качество обучения катастрофически мало
- статус: отказался, но оставил прозапас для других идей
- вывод: сети важно обучаться не между эпизодами, а прямо внутри эпизода

### параллельные игры с уменьшающимся списком
- играть игры параллельно: вместо одной среды теперь будет список сред (игр) длины `batch_size`, с каждой из них собрать список состояний и подать на вход `agent.act` как батч; обновить список после завершения всех игр
- плюс: баланс опыта и обучения
- плюс: быстро (из-за `agent.act`)
- минус: возможен просед по качеству как в прошлом способе (см. вывод, типа много опыта приходит за раз)
- минус: `agent.act` получает батчи разного размера если завершилась только часть игр
- плюс: не нужно переписывать `Trainer`
- статус: пока самое эффективное решение
- вывод: обучение прямо по ходу нескольких эпизодов эффективно

### пареллельные игры со списком постоянного размера
- все как в прошлом способе, но теперь трекаем завершение какой-то игры и тут же заменяем на новую
- плюс: еще эффективнее, т.к. теперь `agent.act` получает батчи одинакого размера
- минус: нужно сильно изменить `Trainer`
- статус: пока не реализовал

## Многошаговая оценка таргета

Формула:

$$
Q_{t:t+n}=R_t+\gamma R_{t+1}+\ldots+\gamma^{n-1}R_{t+n-1}+\gamma^n\max_a Q(S_{t+n},a).
$$

Призвана уменьшить статистические свойства оценки таргета.

- плюс: очень легко реализовать для случая последовательных игр: достаточно указать аргумент `Nstep` в конструкторе `cpprb` и все! суммируется само
- минус: для параллельных игр нужно сохранять в replay буфер сразу целый эпизод, это сразу рушит главное открытие идеи пареллельных игр (что обучаться нужно внутри параллельных эпизодов)
- минус: нужно неиронично реализовывать для параллельных игр когда высоки шансы того что не выстрелит
- минус: нужно еще проводить эксперименты и искать нормальные `n_step` и `gamma`
- вердикт: пока делать это не буду