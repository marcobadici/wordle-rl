Планы:
1. Игры батчами

нужно обеспечить баланс между количеством поступающего опыта (replays) и размером батча обучения
- если опыта будет поступать меньше чем один батч, то сеть будет обучаться на одних и тех же примерах много раз
- если опыта будет поступать больше чем один батч, то сеть будет просто не успевать учиться на всех примерах и память будет тратиться в пустую

примеры, как можно реализовать:
- исходная реализация
    - игры проходят последовательно, на каждом шаге функция `agent.act` вызывается для одного стейта, обучение через каждые два шага
    - минус: нет баланса реплеев и обучения
    - минус: медленно (из-за `agent.act`)
    - плюс: обучается и дает 90% на задаче 100/100
    - итог: пока придерживаюсь
- последовательный способ
    - играть последовательно столько игр, чтобы набралось `batch_size` реплеев, и после этого обучать
    - плюс: баланс опыта и обучения
    - минус: функция `agent.act` все равно обрабатывает не батч стейтов, а последовательно каждый стейт
    - минус: качество обучения катастрофически мало
    - вердикт: отказался
    - вывод: сети важно обучаться не между играми, а во время
- параллельные игры с уменьшающимся списком
    - играть игры параллельно: вместо одной среды теперь будет список сред (игр) длины `batch_size`, с каждой из них собрать список состояний и подать на вход `agent.act` как батч; обновить список после завершения всех игр
    - плюс: баланс опыта и обучения
    - плюс: быстро (из-за `agent.act`)
    - минус: возможен просед по качеству как в прошлом способе (см. вывод, типа много опыта приходит за раз)
    - минус: `agent.act` получает батчи разного размера если завершилась только часть игр
    - плюс: не нужно переписывать `Trainer`
- пареллельные игры со списком постоянного размера
    - все как в прошлом способе, но теперь трекаем завершение какой-то игры и тут же заменяем на новую
    - плюс: еще эффективнее, т.к. теперь `agent.act` получает батчи одинакого размера
    - минус: нужно переписывать логирование в `Trainer`, т.к. теперь `batch_scores` и `batch_wins` приходят неравномерно

2. убрать $\beta$ из replay buffer

этот параметр компенсирует какое-то там смещение

- кажется он нужен только для double dqn, а сейчас только портит обучение и усложняет исследование