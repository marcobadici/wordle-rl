# Идеи и планы

## Исследование архитектуры сети

Параметры, которые нужно подобрать:
1. посмотреть число слоев
2. добавить батчнормы

### попробовать оптюну

- плюс: красивые графики и простая интерпретация, высокая точность оптимизации
- минус: это будет считаться миллион времени, НО: мейби можно придумать какой-то простой и быстрый бенчмарк (например, аккураси на тестовом геймплее 100/100 после одной минуты обучения)

### использовать эталонное решение

1. реализовать решение 3blue1brown
2. нагенерить буфер реплеев
3. к распределениям q-функции применить софтмакс
4. действия эталона считать как вырожденное распределение
5. обучить на кросс-энтропию

- это можно использовать как еще один бенчмарк в оптюне, который косвенно проверяет что вообще способна выучить такая архитектура (например, аккураси на тестовом геймплее ПОЛНОЙ ЗАДАЧИ после минуты обучения)
- минус: надо реализовать...
- минус: это долго реализовывать...

## Игры батчами

нужно обеспечить баланс между количеством поступающего опыта (replays) и размером батча обучения
- если опыта будет поступать меньше чем один батч, то сеть будет обучаться на одних и тех же примерах много раз
- если опыта будет поступать больше чем один батч, то сеть будет просто не успевать учиться на всех примерах и память будет тратиться в пустую

### исходная реализация

- игры проходят последовательно, на каждом шаге функция `agent.act` вызывается для одного стейта, обучение через каждые два шага
- минус: нет баланса реплеев и обучения
- минус: медленно (из-за `agent.act`)
- плюс: обучается и дает 90% на задаче 100/100
- статус: отказался
- вывод: баланс действительно нужен

### последовательный способ

- играть последовательно столько игр, чтобы набралось `batch_size` реплеев, и после этого обучать
- плюс: баланс опыта и обучения
- минус: функция `agent.act` все равно обрабатывает не батч стейтов, а последовательно каждый стейт
- минус: качество обучения катастрофически мало
- статус: отказался, но оставил прозапас для других идей
- вывод: сети важно обучаться не между эпизодами, а прямо внутри эпизода

### параллельные игры с уменьшающимся списком
- играть игры параллельно: вместо одной среды теперь будет список сред (игр) длины `batch_size`, с каждой из них собрать список состояний и подать на вход `agent.act` как батч; обновить список после завершения всех игр
- плюс: баланс опыта и обучения
- плюс: быстро (из-за `agent.act`)
- минус: возможен просед по качеству как в прошлом способе (см. вывод, типа много опыта приходит за раз)
- минус: `agent.act` получает батчи разного размера если завершилась только часть игр
- плюс: не нужно переписывать `Trainer`
- статус: пока самое эффективное решение
- вывод: обучение прямо по ходу нескольких эпизодов эффективно

### пареллельные игры со списком постоянного размера
- все как в прошлом способе, но теперь трекаем завершение какой-то игры и тут же заменяем на новую
- плюс: еще эффективнее, т.к. теперь `agent.act` получает батчи одинакого размера
- минус: нужно сильно изменить `Trainer`
- статус: пока не реализовал

## Многошаговая оценка таргета

Формула:
$$
Q_{t:t+n}=R_t+\gamma R_{t+1}+\ldots+\gamma^{n-1}R_{t+n-1}+\gamma^n\max_a Q(S_{t+n},a).
$$

Призвана уменьшить статистические свойства оценки таргета.

- плюс: очень легко реализовать для случая последовательных игр: достаточно указать аргумент `Nstep` в конструкторе `cpprb` и все! суммируется само
- минус: для параллельных игр нужно сохранять в replay буфер сразу целый эпизод, это сразу рушит главное открытие идеи пареллельных игр (что обучаться нужно внутри параллельных эпизодов)
- минус: нужно неиронично реализовывать для параллельных игр когда высоки шансы того что не выстрелит
- минус: нужно еще проводить эксперименты и искать нормальные `n_step` и `gamma`
- вердикт: пока делать это не буду