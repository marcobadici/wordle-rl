Планы:
1. Игры батчами

нужно обеспечить баланс между количеством поступающего опыта (replays) и размером батча обучения
- если опыта будет поступать меньше чем один батч, то сеть будет обучаться на одних и тех же примерах много раз
- если опыта будет поступать больше чем один батч, то сеть будет просто не успевать учиться на всех примерах и память будет тратиться в пустую

2. убрать $\beta$ из replay buffer

этот параметр компенсирует какое-то там смещение

- кажется он нужен только для double dqn, а сейчас только портит обучение и усложняет исследование