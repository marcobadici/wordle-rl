# Идеи и планы

## Как побить сотку

Испробовано:
- Увеличить буфер и пиклить его после каждого обучения
- Сменить стратегию обучения и тестирования
- fine-tune 99_27 решение
- награда за каждую букву
- наказание за повторение букв
- наказание за повторение зеленых букв

В планах: 
- 130 бекбон для stimulate-решений
- 130-model as a backbone for full-model
- Выбирать слово по вагонами
- Дополнить список ответов и попыток новыми словами и надеятся что он снова приблизится к 100 и захватит все слова из оригинальной игры.
- Использование 130-модели в качестве бекбона для 4767-модели
- Бенчмарки
    - Слова из 6-10 букв
- Обучить эмбеддинги по хорошему:
    - решение проблемы контекстов:
        - контексты: для каждой буквы алфавита всевозможные `context_size` слов с общей буквой
    - негатив семплинг
    - скалярное произведение
    - сигмоида
    - поменьше чем 10 компонент?
- distributional RL
    1. Провести глубокий анализ целью которого будет либо гипотеза о том почему это должно сработать, либо веская причина почему это не имеет смысла

## Текущие результаты и мысли

- Чтобы достичь высокий win rate кажется необязательно использовать multi-stage, т.к. на задача 4к норм обучилось до 97%.
- Надо попробовать обучить с нуля на полной задаче.
- Есть гипотеза, что из-за multi-stage агент не находит оптимальное открывающее слово (сейчас он использует CITAL).
- Для достижения 97.62% на полной задаче нужно было уменьшить эпсилон с 0,05 до 0,001.
- Возможно для добивания финального качества нужно выбрать менее консервативный $\tau$ 
- Для достижения 99.27% на полной задаче нужно было уменьшить шаг оптимизатора с 5e-4 до 1e-4
- возможно пора исследовать архитектуру сети (батчнормы и тп)

Способы добить 100%, которые не сработали:
- в одной среде из списка выдавать только такие слова, которые агент не угадал во время последнего тестирования
- еще больше уменьшить шаг оптимизатора (до 5е-5)
- сменить оптимизатор c Adam на AdamW или SGD

## Бенчмарки

В порядке увеличения сложности реализации:
- угадывать не только 2315 слов, а все 12к
- больше чем 5 букв
- dordle, quordle, и тп
- fible 

## Эмбеддинги для списка слов (пространства действий)

### Решение andrewkho@git

Предлагается обучить Q-сеть предсказывать не сразу вектор из 13K значений Q-функции, а предсказывать вектор из 26*5 значений: сеть предсказывает ценность каждой из 26 букв алфавита быть на каждой из 5 позиций в слове. Тогда значение Q-функции для данного слова при данном состоянии среды определяется как сумма ценностей его букв.

Более строго. Пусть
- `alph` --- алфавит размера $m=26$
- `guesses` --- список длины $n=12972$ валидных слов из $q=5$ букв
- $W\in\{0,1\}^{n\times mq}$ --- OneHot-кодированный список слов, где `W[i, j*26+k] = guesses[i][j] == alph[k]`, т.е. индикатор того, что $j$-ая буква $i$-го слова является $k$-ой буквой алфавита
- $s$ --- состояние среды, $a_\theta(s)\in\mathbb{R}^{mq}$ --- выход нейросети.

Тогда значения $Q$-функции вычисляются как

$$
Q(s)=Wa_\theta(s)\in\mathbb{R}^{n}.
$$

Идею такого решения можно описать так: представить каждое действие агента как сумму некоторых других действий, тогда ценность исходного действия равна сумме ценностей поддействий. В данном случае выбор слова был представлен как результат выборов каждой его буквы по отдельности. Главное, чтобы разбиение действия на поддействия можно было реализовать дифференцируемыми операциями.

У решения andrewkho@git один недостаток. Цель начальных ходов в Wordle --- получить информацию о буквах и позициях, которые еще не вводились ранее, значит, стоит определять поддействия как это самое тестирование. Решение andrewkho@git тестирует комбинацию (буква, позиция), но нет связи между (буква1, позиция1) и (буква2, позиция2). В связи с этим вот идеи как бить слово так, чтобы учесть взаимодействие:
- Пары соседних букв: `where -> wh, he, er, re`
    - количество возможных пар букв равно $m^2=676$, всего позиций $q-1$, значит размер эмбеддинга $h=(q-1)m^2=4\cdot676=2704$
    - минус: в таком решении позиция1 и позиция2 обязательно соседние, кажется что это большое ограничение, идеальный игрок должен уметь тестировать любые пары позиций
- Все пары букв: `where -> wh, we, wr, we, he, hr, he, er, ee, re`
    - способов выбрать пару ${q\choose 2}=0.5q(q-1)$, поэтому размерность эмбеддинга $h=0.5q(q-1)m^2=0.5\cdot5\cdot4\cdot676=6760$
    - это решение мне пока кажется идеальным (из-за объяснений в терминах буква1, позиция1)

Описанное решение обладает только одним недостатком: размер эмбеддинга растет квадратично по размеру алфавита и по длине слова. Например, если играть с русскими словами из 10 букв, то эмбеддинг потребует $0.5\cdot10\cdot9\cdot32^2=46080$ (при том что плюс-минус общеупотребительных слов из 10 букв на русском языке ненамного больше?).

Рассмотрим вопрос универсальности. Такой эмбеддинг позволит агенту обучиться решать задачу игры в Wordle весьма обобщенно: ведь теперь он учится не просто выбирать слово из списка, а тестировать буквы и позиции. Поэтому, если к списку тестовых слов добавить новое, то чтобы найти его функцию ценности достаточно добавить строку его OneHot-кода в матрицу $W$. Это значит, что агент сможет угадывать слова, которые ранее не видел. Правда по-хорошему после усложнения задачи добавлением слов в идеале нужно дообучиться, т.к. функция ценности, аппроксимируемая нейросетью, уделяет должное внимание только тем сочетаниям букв и позиций, которые встречались при обучении.

На описанное решение можно посмотреть как на архитектуру кодировщик---декодировщик: кодировщиком является нейросеть, которая выдает вектор ценности для поддействий длины $h$, а декодировщиком --- матрица OneHot-кодов, выдающая вектор длины $n$, по сути линейный слой.

Можно ли обучить нейросеть бить действие на поддействия? Что мы требуем: чтобы она выдавала вектор длины $h$, и сама его декодировала. По сути для этого надо в обычной полносвязной нейросети с $n$ выходами убрать функцию активации перед последним слоем.

Можно попытаться придумать декодировщик, но как мне кажется, любое решение где на выходе стоит $n$ чисел не стоит того, чтобы продолжать о нем говорить.

### Использование непрерывного эмбеддинга.

Сеть которая осуществляет преобразование вида

$$
a_\theta(s)=Q(s, \arg\max_eQ(s,a(e))),
$$

где $e\in\R^h$ --- эмбеддинг (вектор с вещественными числами), $a(e)$ --- слово, эмбеддинг которого ближе всех к данному эмбеддингу (по метрике).
- Режим теста. Вход $e$ будет в некотором смысле пустышкой при запуске теста, а после завершения теста в нем уже будет храниться эмбединг-ответ.
    1. Проход вперед с входом $s,e_0\to$ получили значение $Q(s,e_0)$.
    2. Затем нужен проход назад, чтобы пробросить градиент до входа $e$, при этом нужно отключить обновление параметров сети.
    3. Получили градиент по $e$ --- делаем один шаг оптимизации и получаем $e_1$.
    4. Подаем $s,e_1$ на вход сети для прохода вперед, получаем $Q(s,e_1)$ повторяем шаги 2-3. Так делаем, к примеру, 20 раз методом L-BFGS.
    5. Выбираем действие $a(e)$
- Режим обучения. 
    1. Делаем все то же самое
    2. Для полученного $Q(s,e^*)$ считаем MSE (согласно архитектуре DQN) и теперь уже обновляем параметры сети.

Авторы оригинальной [статьи](https://arxiv.org/pdf/1609.07152.pdf) предлагают добавить энтропийный регуляризатор и решать двойственную задачу с помощью метода Ньютона с проекциями. 
- нужно немного дополнить архитектуру сети
- в их тестах с DQN пространство действий не больше 17, т.е. у меня может сработать хуже
- в торче нету метода Ньютона с проекциями...
- кажется действительно придется использовать L-BFGS: для этого надо будет создать отдельный оптмизатор и в качестве параметров дать ему вход $y$ (обозн. из статьи) в качестве параметров

### Какие непрерывные эмбеддинги использовать

Пока идея такая: сформировать датасет из пар слов, у которых есть общие буквы, и обучить автокодировщик предсказывать по первому слову из пары второе слово из этой же пары (как в skip-gram: на вход OneHot длины $n$, на выход софтмакс длины $n$, обучаться на кросс-энтропии). Скрытый слой имеет размерность $h$, тогда его веса будут эмбеддингами.

Возможные архитектуры: [git](https://github.com/OlgaChernytska/word2vec-pytorch/blob/87b0418fcc6a0f5b8ac96698f6fc1079014b4615/utils/model.py#L30).

## Исследование архитектуры сети

Параметры, которые нужно подобрать:
1. посмотреть число слоев
2. добавить батчнормы
3. нужно ли домножать $z_i$ на $\tilde Wu_i+\tilde b_i$ в ICNN (потому что пока что с одной стороны что модель будто убогая, с другой стороны параметров примерно столько же, сколько было) 

### попробовать оптюну

- плюс: красивые графики и простая интерпретация, высокая точность оптимизации
- минус: это будет считаться миллион времени, НО: мейби можно придумать какой-то простой и быстрый бенчмарк (например, аккураси на тестовом геймплее 100/100 после одной минуты обучения)

### использовать эталонное решение

1. реализовать решение 3blue1brown
2. нагенерить буфер реплеев
3. к распределениям q-функции применить софтмакс
4. действия эталона считать как вырожденное распределение
5. обучить на кросс-энтропию

- это можно использовать как еще один бенчмарк в оптюне, который косвенно проверяет что вообще способна выучить такая архитектура (например, аккураси на тестовом геймплее ПОЛНОЙ ЗАДАЧИ после минуты обучения)
- минус: надо реализовать...
- минус: это долго реализовывать...

## Игры батчами

нужно обеспечить баланс между количеством поступающего опыта (replays) и размером батча обучения
- если опыта будет поступать меньше чем один батч, то сеть будет обучаться на одних и тех же примерах много раз
- если опыта будет поступать больше чем один батч, то сеть будет просто не успевать учиться на всех примерах и память будет тратиться в пустую

### исходная реализация

- игры проходят последовательно, на каждом шаге функция `agent.act` вызывается для одного стейта, обучение через каждые два шага
- минус: нет баланса реплеев и обучения
- минус: медленно (из-за `agent.act`)
- плюс: обучается и дает 90% на задаче 100/100
- статус: отказался
- вывод: баланс действительно нужен

### последовательный способ

- играть последовательно столько игр, чтобы набралось `batch_size` реплеев, и после этого обучать
- плюс: баланс опыта и обучения
- минус: функция `agent.act` все равно обрабатывает не батч стейтов, а последовательно каждый стейт
- минус: качество обучения катастрофически мало
- статус: отказался, но оставил прозапас для других идей
- вывод: сети важно обучаться не между эпизодами, а прямо внутри эпизода

### параллельные игры с уменьшающимся списком
- играть игры параллельно: вместо одной среды теперь будет список сред (игр) длины `batch_size`, с каждой из них собрать список состояний и подать на вход `agent.act` как батч; обновить список после завершения всех игр
- плюс: баланс опыта и обучения
- плюс: быстро (из-за `agent.act`)
- минус: возможен просед по качеству как в прошлом способе (см. вывод, типа много опыта приходит за раз)
- минус: `agent.act` получает батчи разного размера если завершилась только часть игр
- плюс: не нужно переписывать `Trainer`
- статус: пока самое эффективное решение
- вывод: обучение прямо по ходу нескольких эпизодов эффективно

### пареллельные игры со списком постоянного размера
- все как в прошлом способе, но теперь трекаем завершение какой-то игры и тут же заменяем на новую
- плюс: еще эффективнее, т.к. теперь `agent.act` получает батчи одинакого размера
- минус: нужно сильно изменить `Trainer`
- статус: пока не реализовал

## Многошаговая оценка таргета

Формула:

$$
Q_{t:t+n}=R_t+\gamma R_{t+1}+\ldots+\gamma^{n-1}R_{t+n-1}+\gamma^n\max_a Q(S_{t+n},a).
$$

Призвана уменьшить статистические свойства оценки таргета.

- плюс: очень легко реализовать для случая последовательных игр: достаточно указать аргумент `Nstep` в конструкторе `cpprb` и все! суммируется само
- минус: для параллельных игр нужно сохранять в replay буфер сразу целый эпизод, это сразу рушит главное открытие идеи пареллельных игр (что обучаться нужно внутри параллельных эпизодов)
- минус: нужно неиронично реализовывать для параллельных игр когда высоки шансы того что не выстрелит
- минус: нужно еще проводить эксперименты и искать нормальные `n_step` и `gamma`
- вердикт: пока делать это не буду